{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Examen Parcial"
      ],
      "metadata": {
        "id": "bQZKWskrK7vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 1"
      ],
      "metadata": {
        "id": "Surz5MxqLuV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 1"
      ],
      "metadata": {
        "id": "QvmlvmQVL7Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import collections\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "#Corpus de entrenamiento no tiene la palabra \"is\" dado que\n",
        "#el vocabulario especificado en el enunciado no lo contiene\n",
        "#y la probabilidad de ngramas conteniendo dicha palabra va a ser\n",
        "#interpolada\n",
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "test = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "\n",
        "tokens = [text.split() for text in corpus]\n",
        "# print(tokens)\n",
        "\n",
        "# Implementación de modelos N-grama\n",
        "class NGramModel:\n",
        "    def __init__(self, n: int):\n",
        "        self.n = n\n",
        "        self.ngram_counts = collections.Counter()\n",
        "        self.context_counts = collections.Counter()\n",
        "        self.vocab = set()\n",
        "        self.total_ngrams = 0\n",
        "\n",
        "    def train(self, corpus: List[List[str]]):\n",
        "        for document in corpus:\n",
        "            if(self.n == 1):\n",
        "                tokens = ['<s>'] + document + ['</s>']\n",
        "            else:\n",
        "                tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
        "            # print(tokens\n",
        "            self.vocab.update(tokens)\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                ngram = tuple(tokens[i:i + self.n])\n",
        "                context = tuple(tokens[i:i + self.n - 1])\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "                self.total_ngrams += 1\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...], k=0) -> float:\n",
        "        count = self.ngram_counts.get(ngram, 0)\n",
        "        context = ngram[:-1]\n",
        "        context_count = self.context_counts.get(context, 0)\n",
        "        if context_count == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return (count + k) / (context_count + k*self.total_ngrams)\n",
        "\n",
        "    def get_sentence_probability(self, sentence: List[str], k=0) -> float:\n",
        "        tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
        "        probability = 1.0\n",
        "        for i in range(len(tokens) - self.n + 1):\n",
        "            ngram = tuple(tokens[i:i + self.n])\n",
        "            prob = self.get_ngram_prob(ngram, k)\n",
        "            print(f\"P({ngram[-1]}|{ngram[:-1]}) = {prob}\")\n",
        "            if prob > 0:\n",
        "                probability *= prob\n",
        "            else:\n",
        "                # Asignamos una pequeña probabilidad para evitar cero\n",
        "                probability *= 1e-6\n",
        "        return probability\n",
        "\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.train(tokens)\n",
        "print(bigram_model.vocab)\n",
        "\n",
        "print(\"a) Probabilidades de bigramas SIN SUAVIZADO\")\n",
        "for text in test:\n",
        "    prob = bigram_model.get_sentence_probability(text.split())\n",
        "    print(f\"Texto: {text}, P: {prob}\")\n",
        "print(\"-\"*10)\n",
        "print(\"b) Probabilidades de bigramas CON SUAVIZADO ADD-ONE\")\n",
        "for text in test:\n",
        "    prob = bigram_model.get_sentence_probability(text.split(), 1)\n",
        "    print(f\"Texto: {text}, P: {prob}\")\n",
        "\n",
        "print(\"-\"*10)\n",
        "print(\"c) Probabilidades de bigramas CON SUAVIZADO ADD-K\")\n",
        "for k in [0.05, 0.15]:\n",
        "  print(f\"K: {k}\")\n",
        "  for text in test:\n",
        "    prob = bigram_model.get_sentence_probability(text.split(), k)\n",
        "    print(f\"-Texto: {text}, P: {prob}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqTm0PRfLt4G",
        "outputId": "8097e876-e705-44aa-b410-dc4d06315049"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a', 'useful', 'model', '<s>', '</s>', 'models', 'wrong', 'some', 'are', 'all'}\n",
            "a) Probabilidades de bigramas SIN SUAVIZADO\n",
            "P(all|('<s>',)) = 0.3333333333333333\n",
            "P(models|('all',)) = 1.0\n",
            "P(are|('models',)) = 1.0\n",
            "P(wrong|('are',)) = 0.5\n",
            "P(</s>|('wrong',)) = 1.0\n",
            "Texto: all models are wrong, P: 0.16666666666666666\n",
            "P(a|('<s>',)) = 0.3333333333333333\n",
            "P(model|('a',)) = 1.0\n",
            "P(is|('model',)) = 0.0\n",
            "P(wrong|('is',)) = 0.0\n",
            "P(</s>|('wrong',)) = 1.0\n",
            "Texto: a model is wrong, P: 3.333333333333333e-13\n",
            "P(some|('<s>',)) = 0.3333333333333333\n",
            "P(models|('some',)) = 1.0\n",
            "P(are|('models',)) = 1.0\n",
            "P(useful|('are',)) = 0.5\n",
            "P(</s>|('useful',)) = 1.0\n",
            "Texto: some models are useful, P: 0.16666666666666666\n",
            "----------\n",
            "b) Probabilidades de bigramas CON SUAVIZADO ADD-ONE\n",
            "P(all|('<s>',)) = 0.11764705882352941\n",
            "P(models|('all',)) = 0.13333333333333333\n",
            "P(are|('models',)) = 0.1875\n",
            "P(wrong|('are',)) = 0.125\n",
            "P(</s>|('wrong',)) = 0.1875\n",
            "Texto: all models are wrong, P: 6.893382352941176e-05\n",
            "P(a|('<s>',)) = 0.11764705882352941\n",
            "P(model|('a',)) = 0.13333333333333333\n",
            "P(is|('model',)) = 0.06666666666666667\n",
            "P(wrong|('is',)) = 0.0\n",
            "P(</s>|('wrong',)) = 0.1875\n",
            "Texto: a model is wrong, P: 1.9607843137254898e-10\n",
            "P(some|('<s>',)) = 0.11764705882352941\n",
            "P(models|('some',)) = 0.13333333333333333\n",
            "P(are|('models',)) = 0.1875\n",
            "P(useful|('are',)) = 0.125\n",
            "P(</s>|('useful',)) = 0.13333333333333333\n",
            "Texto: some models are useful, P: 4.901960784313725e-05\n",
            "----------\n",
            "c) Probabilidades de bigramas CON SUAVIZADO ADD-K\n",
            "K: 0.05\n",
            "P(all|('<s>',)) = 0.28378378378378377\n",
            "P(models|('all',)) = 0.6176470588235293\n",
            "P(are|('models',)) = 0.7592592592592592\n",
            "P(wrong|('are',)) = 0.3888888888888889\n",
            "P(</s>|('wrong',)) = 0.7592592592592592\n",
            "-Texto: all models are wrong, P: 0.03929467321351852\n",
            "P(a|('<s>',)) = 0.28378378378378377\n",
            "P(model|('a',)) = 0.6176470588235293\n",
            "P(is|('model',)) = 0.029411764705882353\n",
            "P(wrong|('is',)) = 0.0\n",
            "P(</s>|('wrong',)) = 0.7592592592592592\n",
            "-Texto: a model is wrong, P: 3.914165030081984e-09\n",
            "P(some|('<s>',)) = 0.28378378378378377\n",
            "P(models|('some',)) = 0.6176470588235293\n",
            "P(are|('models',)) = 0.7592592592592592\n",
            "P(useful|('are',)) = 0.3888888888888889\n",
            "P(</s>|('useful',)) = 0.6176470588235293\n",
            "-Texto: some models are useful, P: 0.03196568107900286\n",
            "K: 0.15\n",
            "P(all|('<s>',)) = 0.22549019607843138\n",
            "P(models|('all',)) = 0.3709677419354838\n",
            "P(are|('models',)) = 0.524390243902439\n",
            "P(wrong|('are',)) = 0.2804878048780488\n",
            "P(</s>|('wrong',)) = 0.524390243902439\n",
            "-Texto: all models are wrong, P: 0.006451890689668518\n",
            "P(a|('<s>',)) = 0.22549019607843138\n",
            "P(model|('a',)) = 0.3709677419354838\n",
            "P(is|('model',)) = 0.04838709677419355\n",
            "P(wrong|('is',)) = 0.0\n",
            "P(</s>|('wrong',)) = 0.524390243902439\n",
            "-Texto: a model is wrong, P: 2.122501369777118e-09\n",
            "P(some|('<s>',)) = 0.22549019607843138\n",
            "P(models|('some',)) = 0.3709677419354838\n",
            "P(are|('models',)) = 0.524390243902439\n",
            "P(useful|('are',)) = 0.2804878048780488\n",
            "P(</s>|('useful',)) = 0.3709677419354838\n",
            "-Texto: some models are useful, P: 0.004564240750455672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "test = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "tokens = [text.split() for text in corpus]\n",
        "\n",
        "class BackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel]):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
        "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for model in self.models:\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return prob\n",
        "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
        "        return 1e-6\n",
        "\n",
        "class StupidBackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel], alpha: float = 0.4):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
        "        self.alpha = alpha    # Factor de escala fijo\n",
        "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for i, model in enumerate(self.models):\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return (self.alpha ** i) * prob\n",
        "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
        "        return (self.alpha ** len(self.models)) * (1.0 / len(self.vocab))\n",
        "\n",
        "\n",
        "unigram_model = NGramModel(n=1)\n",
        "unigram_model.train(tokens)\n",
        "\n",
        "bigram_model = NGramModel(n=2)\n",
        "bigram_model.train(tokens)\n",
        "\n",
        "backoff_model = BackoffNGramModel(n=2, models=[bigram_model, unigram_model])\n",
        "stupid_backoff_model = StupidBackoffNGramModel(n=2, models=[bigram_model, unigram_model], alpha=0.4)\n",
        "\n",
        "\n",
        "print(\"d.1) Probabilidades con backoff\")\n",
        "for text in test:\n",
        "    tokens_test = ['<s>'] * (2 - 1) + text.split() + ['</s>']\n",
        "    print(f\"Texto: {tokens_test}\")\n",
        "    for i in range(len(tokens_test) - 2 + 1):\n",
        "      ngram = tuple(tokens_test[i:i + 2])\n",
        "      # print(ngram)\n",
        "      prob = backoff_model.get_ngram_prob(ngram)\n",
        "      print(f\"N-grama: {ngram}, P: {prob}\")\n",
        "\n",
        "print(\"\\nd.2) Probabilidades con stupid backoff\")\n",
        "for text in test:\n",
        "    tokens_test = ['<s>'] * (2 - 1) + text.split() + ['</s>']\n",
        "    print(f\"Texto: {tokens_test}\")\n",
        "    for i in range(len(tokens_test) - 2 + 1):\n",
        "      ngram = tuple(tokens_test[i:i + 2])\n",
        "      # print(ngram)\n",
        "      prob = stupid_backoff_model.get_ngram_prob(ngram)\n",
        "      print(f\"N-grama: {ngram}, P: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2ZGfi1VlcV",
        "outputId": "e8cea4b0-e365-4f2c-a168-9f80bf480fc9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d.1) Probabilidades con backoff\n",
            "Texto: ['<s>', 'all', 'models', 'are', 'wrong', '</s>']\n",
            "N-grama: ('<s>', 'all'), P: 0.3333333333333333\n",
            "N-grama: ('all', 'models'), P: 1.0\n",
            "N-grama: ('models', 'are'), P: 1.0\n",
            "N-grama: ('are', 'wrong'), P: 0.5\n",
            "N-grama: ('wrong', '</s>'), P: 1.0\n",
            "Texto: ['<s>', 'a', 'model', 'is', 'wrong', '</s>']\n",
            "N-grama: ('<s>', 'a'), P: 0.3333333333333333\n",
            "N-grama: ('a', 'model'), P: 1.0\n",
            "N-grama: ('model', 'is'), P: 1e-06\n",
            "N-grama: ('is', 'wrong'), P: 0.11764705882352941\n",
            "N-grama: ('wrong', '</s>'), P: 1.0\n",
            "Texto: ['<s>', 'some', 'models', 'are', 'useful', '</s>']\n",
            "N-grama: ('<s>', 'some'), P: 0.3333333333333333\n",
            "N-grama: ('some', 'models'), P: 1.0\n",
            "N-grama: ('models', 'are'), P: 1.0\n",
            "N-grama: ('are', 'useful'), P: 0.5\n",
            "N-grama: ('useful', '</s>'), P: 1.0\n",
            "\n",
            "d.2) Probabilidades con stupid backoff\n",
            "Texto: ['<s>', 'all', 'models', 'are', 'wrong', '</s>']\n",
            "N-grama: ('<s>', 'all'), P: 0.3333333333333333\n",
            "N-grama: ('all', 'models'), P: 1.0\n",
            "N-grama: ('models', 'are'), P: 1.0\n",
            "N-grama: ('are', 'wrong'), P: 0.5\n",
            "N-grama: ('wrong', '</s>'), P: 1.0\n",
            "Texto: ['<s>', 'a', 'model', 'is', 'wrong', '</s>']\n",
            "N-grama: ('<s>', 'a'), P: 0.3333333333333333\n",
            "N-grama: ('a', 'model'), P: 1.0\n",
            "N-grama: ('model', 'is'), P: 0.016000000000000004\n",
            "N-grama: ('is', 'wrong'), P: 0.047058823529411764\n",
            "N-grama: ('wrong', '</s>'), P: 1.0\n",
            "Texto: ['<s>', 'some', 'models', 'are', 'useful', '</s>']\n",
            "N-grama: ('<s>', 'some'), P: 0.3333333333333333\n",
            "N-grama: ('some', 'models'), P: 1.0\n",
            "N-grama: ('models', 'are'), P: 1.0\n",
            "N-grama: ('are', 'useful'), P: 0.5\n",
            "N-grama: ('useful', '</s>'), P: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 2"
      ],
      "metadata": {
        "id": "fkN5xxDJL8uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ReZ13BSzNXio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def calculate_NC(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[int, int]:\n",
        "    count_of_counts = collections.Counter()\n",
        "    for count in ngram_counts.values():\n",
        "        count_of_counts[count] += 1\n",
        "    return count_of_counts\n",
        "\n",
        "def sort_NC(NC: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    counts = np.array(list(NC.keys()))\n",
        "    frequencies = np.array([NC[count] for count in counts])\n",
        "    sorted_indices = np.argsort(counts)\n",
        "    return counts[sorted_indices], frequencies[sorted_indices]\n",
        "\n",
        "\n",
        "# Suavizado Good-Turing\n",
        "def good_turing_discounting(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    # Calculamos N(C)\n",
        "    NC = calculate_NC(ngram_counts)\n",
        "    counts, frequencies = sort_NC(NC)\n",
        "\n",
        "    # Ajuste de conteos\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    max_count = max(counts)\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count < max_count:\n",
        "            Nc = NC[count]\n",
        "            Nc1 = NC.get(count + 1, 0)\n",
        "            if Nc > 0:\n",
        "                C_star = (count + 1) * (Nc1 / Nc)\n",
        "                adjusted_counts[ngram] = C_star\n",
        "            else:\n",
        "                adjusted_counts[ngram] = count\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count  # Para conteos máximos, no ajustamos\n",
        "    return adjusted_counts\n",
        "\n",
        "adjusted_bigram_counts = good_turing_discounting(bigram_model.ngram_counts)\n",
        "\n",
        "#Cálculo de probabilidades ajustadas\n",
        "def calculate_probabilities(adjusted_counts: Dict[Tuple[str, ...], float], n_minus1_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    probabilities = {}\n",
        "    for ngram, adjusted_count in adjusted_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        context_count = n_minus1_counts.get(context, sum(n_minus1_counts.values()))\n",
        "        probability = adjusted_count / context_count if context_count > 0 else 0.0\n",
        "        probabilities[ngram] = probability\n",
        "    return probabilities\n",
        "\n",
        "def probability_of_unseen(NC: Dict[int, int], total_ngrams: int) -> float:\n",
        "    N1 = NC.get(1, 0)\n",
        "    return N1 / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "def sentence_probability(sentence: str, bigram_probabilities: Dict[Tuple[str, str], float], P_unseen: float) -> float:\n",
        "    tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "    probability_log_sum = 0.0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigram = (tokens[i], tokens[i+1])\n",
        "        prob = bigram_probabilities.get(bigram, P_unseen)\n",
        "        probability_log_sum += math.log(prob) if prob > 0 else float('-inf')\n",
        "    return math.exp(probability_log_sum)\n",
        "\n",
        "# Calculamos las probabilidades ajustadas para bigramas\n",
        "\n",
        "NC_ngram = calculate_NC(unigram_model.ngram_counts)\n",
        "print(\"Conteos de unigramas\")\n",
        "for ngram, count in unigram_model.ngram_counts.items():\n",
        "    print(f\"{ngram}: {count}\")\n",
        "\n",
        "print(\"Conteo de conteos\")\n",
        "for count, freq in NC_ngram.items():\n",
        "    print(f\"r = {count}: {freq}\")\n",
        "\n",
        "adjusted_counts = good_turing_discounting(unigram_model.ngram_counts)\n",
        "print(\"Conteos de unigramas ajustados\")\n",
        "for ngram, count in adjusted_counts.items():\n",
        "    print(f\"{ngram}: {count}\")\n",
        "\n",
        "probabilities = calculate_probabilities(adjusted_counts, unigram_model.ngram_counts)\n",
        "print(\"Probabilidades de unigramas ajustados\")\n",
        "for ngram, count in probabilities.items():\n",
        "    print(f\"{ngram}: {count}\")\n",
        "\n",
        "total_ngrams = sum(bigram_model.ngram_counts.values())\n",
        "P_unseen_ngram = probability_of_unseen(NC_ngram, total_ngrams)\n",
        "\n",
        "for test_sentence in test:\n",
        "    prob = sentence_probability(test_sentence, probabilities, P_unseen_ngram)\n",
        "    print(f\"La probabilidad de la oración '{test_sentence}' es: {prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTeA72taL9wS",
        "outputId": "4396fe91-5a88-4310-975d-e5765e6d9f17"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conteos de unigramas\n",
            "('<s>',): 3\n",
            "('all',): 1\n",
            "('models',): 2\n",
            "('are',): 2\n",
            "('wrong',): 2\n",
            "('</s>',): 3\n",
            "('a',): 1\n",
            "('model',): 1\n",
            "('some',): 1\n",
            "('useful',): 1\n",
            "Conteo de conteos\n",
            "r = 3: 2\n",
            "r = 1: 5\n",
            "r = 2: 3\n",
            "Conteos de unigramas ajustados\n",
            "('<s>',): 3\n",
            "('all',): 1.2\n",
            "('models',): 2.0\n",
            "('are',): 2.0\n",
            "('wrong',): 2.0\n",
            "('</s>',): 3\n",
            "('a',): 1.2\n",
            "('model',): 1.2\n",
            "('some',): 1.2\n",
            "('useful',): 1.2\n",
            "Probabilidades de unigramas ajustados\n",
            "('<s>',): 0.17647058823529413\n",
            "('all',): 0.07058823529411765\n",
            "('models',): 0.11764705882352941\n",
            "('are',): 0.11764705882352941\n",
            "('wrong',): 0.11764705882352941\n",
            "('</s>',): 0.17647058823529413\n",
            "('a',): 0.07058823529411765\n",
            "('model',): 0.07058823529411765\n",
            "('some',): 0.07058823529411765\n",
            "('useful',): 0.07058823529411765\n",
            "La probabilidad de la oración 'all models are wrong' es: 0.00581045100255846\n",
            "La probabilidad de la oración 'a model is wrong' es: 0.00581045100255846\n",
            "La probabilidad de la oración 'some models are useful' es: 0.00581045100255846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"f) Probabilidades para r=3\")\n",
        "probs = []\n",
        "for ngram, count in probabilities.items():\n",
        "    if(unigram_model.ngram_counts[ngram] != 3):\n",
        "        probs.append(count)\n",
        "        print(f\"{ngram}: {count}\")\n",
        "    else:\n",
        "        probs.append(unigram_model.get_ngram_prob(ngram))\n",
        "        print(f\"{ngram}: {unigram_model.get_ngram_prob(ngram)}\")\n",
        "print(f\"Suma de PROB: {np.sum(probs)}\")\n",
        "\n",
        "\n",
        "print(\"\\nh) Normalizar las probabilidades\")\n",
        "for ngram, count in probabilities.items():\n",
        "    print(f\"{ngram}: {count/np.sum(probs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM1wSwLaiMeA",
        "outputId": "1033c2e1-b729-4256-e678-a4a7135c8eb4"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f) Probabilidades para r=3\n",
            "('<s>',): 0.17647058823529413\n",
            "('all',): 0.07058823529411765\n",
            "('models',): 0.11764705882352941\n",
            "('are',): 0.11764705882352941\n",
            "('wrong',): 0.11764705882352941\n",
            "('</s>',): 0.17647058823529413\n",
            "('a',): 0.07058823529411765\n",
            "('model',): 0.07058823529411765\n",
            "('some',): 0.07058823529411765\n",
            "('useful',): 0.07058823529411765\n",
            "Suma de PROB: 1.0588235294117647\n",
            "\n",
            "h) Normalizar las probabilidades\n",
            "('<s>',): 0.16666666666666669\n",
            "('all',): 0.06666666666666667\n",
            "('models',): 0.1111111111111111\n",
            "('are',): 0.1111111111111111\n",
            "('wrong',): 0.1111111111111111\n",
            "('</s>',): 0.16666666666666669\n",
            "('a',): 0.06666666666666667\n",
            "('model',): 0.06666666666666667\n",
            "('some',): 0.06666666666666667\n",
            "('useful',): 0.06666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 2"
      ],
      "metadata": {
        "id": "zjM6w-SKovbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiU2vfCoKlMB",
        "outputId": "48131178-5c4f-48e8-e12a-ac0109832742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "Counter({'de': 2, 'mi': 1, 'libro,': 1, 'luna': 1, 'pluton,': 1, 'ya': 1, 'esta': 1, 'disponible': 1, 'en': 1, 'todas': 1, 'las': 1, 'librerias': 1, 'habla': 1, 'hispana': 1})\n",
            "[[('mi', 1)], [('libro,', 1)], [('luna', 1)], [('de', 2)], [('pluton,', 1)], [('ya', 1)], [('esta', 1)], [('disponible', 1)], [('en', 1)], [('todas', 1)], [('las', 1)], [('librerias', 1)], [('habla', 1)], [('hispana', 1)]]\n"
          ]
        }
      ],
      "source": [
        "class BrownClustering:\n",
        "  def __init__(self, corpus):\n",
        "      self.corpus = corpus\n",
        "      self.word_count = collections.Counter()\n",
        "      self.word_count.update(self.corpus.lower().split())\n",
        "      self.classes = []\n",
        "\n",
        "  def train(self):\n",
        "      self.classes = [[item] for item in self.word_count.items()]\n",
        "      print(len(self.classes))\n",
        "      print(self.word_count)\n",
        "      print(self.classes)\n",
        "\n",
        "\n",
        "  def get_class(self, word):\n",
        "      for i in range(len(self.classes)):\n",
        "          if word in self.classes[i]:\n",
        "              return i\n",
        "      return None\n",
        "\n",
        "\n",
        "\n",
        "  def prob_word_given_class(self, word):\n",
        "      return self.word_count[word]/len(self.classes[self.get_class(word)])\n",
        "\n",
        "  def prob_class(self, c):\n",
        "      freq = np.sum([w[1] for w in self.classes[c]])\n",
        "      return freq/len(self.word_count)\n",
        "\n",
        "  def prob_sequence(self, list_words):\n",
        "      prob = self.prob_class(self.get_class(list_words[0]))\n",
        "      prob *= self.prob_word_given_class(list_words[1])\n",
        "      for i in range(len(list_words[2:])):\n",
        "          prob *= self.prob_class(self.get_class(list_words[i]))*self.prob_class(self.get_class(list_words[i-1]))\n",
        "          prob *= self.prob_word_given_class(list_words[i])\n",
        "      return prob\n",
        "\n",
        "  def mutual_info(self):\n",
        "\n",
        "\n",
        "\n",
        "corpus = \"Mi libro, luna de pluton, ya esta disponible en todas las librerias de habla hispana\"\n",
        "# corpus = \"Los lenguajes son una parte crucial de la inteligencia humana e importantes para la comunicación humana. Al investigar la comprensión automática y la generación de lenguajes humanos, el procesamiento del lenguaje natural (NLP) ha sido un subcampo central de la investigación en inteligencia artificial. Desde la década de 1950, la tecnología de NLP ha recibido una atención continua por parte de la investigación y se han logrado grandes avances. Hoy en día, la tecnología NLP se está convirtiendo en una parte indispensable de nuestro negocio y de nuestra vida diaria. Por ejemplo, los motores de búsqueda procesan automáticamente billones de documentos a través de Internet, obtienen conocimientos de ellos y responden a las consultas de los usuarios basándose en su comprensión. Los minoristas en línea procesan millones de descripciones de productos y comentarios de usuarios para recomendar el producto más adecuado según la búsqueda de un usuario. Los sistemas de diálogo automático y los sistemas de traducción son cada vez más utilizados para facilitar la comunicación. En los negocios, los motores de análisis de texto han estado reemplazando el trabajo manual en el análisis de grandes cantidades de documentos para una mejor toma de decisiones.\"\n",
        "\n",
        "brown_clustering = BrownClustering(corpus)\n",
        "brown_clustering.train()"
      ]
    }
  ]
}